{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import glob \n",
    "import os\n",
    "import numpy as np\n",
    "import bdpy\n",
    "from PIL import Image\n",
    "import torchvision.models as models\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, batch_size):\n",
    "        super(Generator, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.defc7 = nn.Linear(9919 + 9216, 4096)\n",
    "        self.relu_defc7 = nn.ReLU(0.3)\n",
    "        nn.init.kaiming_normal_(self.defc7.weight, mode='fan_in', nonlinearity='relu')\n",
    "        nn.init.constant_(self.defc7.bias, 0)\n",
    "        \n",
    "\n",
    "        self.defc6 = nn.Linear(4096, 4096)\n",
    "        self.relu_defc6 = nn.ReLU(0.3)\n",
    "        nn.init.kaiming_normal_(self.defc6.weight, mode='fan_in', nonlinearity='relu')\n",
    "        nn.init.constant_(self.defc6.bias, 0)\n",
    "\n",
    "        self.defc5 = nn.Linear(4096, 4096)\n",
    "        self.relu_defc5 = nn.ReLU(0.3)\n",
    "        nn.init.kaiming_normal_(self.defc5.weight, mode='fan_in', nonlinearity='relu')\n",
    "        nn.init.constant_(self.defc5.bias, 0)\n",
    "\n",
    "        self.deconv5 = nn.ConvTranspose2d(256, 256, kernel_size=4, stride=2, padding=1)\n",
    "        self.relu_deconv5 = nn.ReLU(0.3)\n",
    "        nn.init.kaiming_normal_(self.deconv5.weight, a=1.8, mode='fan_out', nonlinearity='relu')\n",
    "        nn.init.constant_(self.deconv5.bias, 0)\n",
    "        \n",
    "        \n",
    "\n",
    "        self.conv5_1 = nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1)  # Convolution\n",
    "        self.relu_conv5_1 = nn.ReLU(0.3)\n",
    "        nn.init.kaiming_normal_(self.conv5_1.weight, a=0.9, mode='fan_out', nonlinearity='relu')\n",
    "        nn.init.constant_(self.conv5_1.bias, 0)\n",
    "\n",
    "        self.deconv4 = nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1)  # Deconvolution\n",
    "        self.relu_deconv4 = nn.ReLU(0.3)\n",
    "        nn.init.kaiming_normal_(self.deconv4.weight, a=1.8, mode='fan_out', nonlinearity='relu')\n",
    "        nn.init.constant_(self.deconv4.bias, 0)\n",
    "\n",
    "        self.conv4_1 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)  # Convolution\n",
    "        self.relu_conv4_1 = nn.ReLU(0.3)\n",
    "        nn.init.kaiming_normal_(self.conv4_1.weight, a=0.9, mode='fan_out', nonlinearity='relu')\n",
    "        nn.init.constant_(self.conv4_1.bias, 0)\n",
    "\n",
    "        self.deconv3 = nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1)  # Deconvolution\n",
    "        self.relu_deconv3 = nn.ReLU(0.3)\n",
    "        nn.init.kaiming_normal_(self.deconv3.weight, a=1.8, mode='fan_out', nonlinearity='relu')\n",
    "        nn.init.constant_(self.deconv3.bias, 0)\n",
    "\n",
    "        self.conv3_1 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)  # Convolution\n",
    "        self.relu_conv3_1 = nn.ReLU(0.3)\n",
    "        nn.init.kaiming_normal_(self.conv3_1.weight, a=0.9, mode='fan_out', nonlinearity='relu')\n",
    "        nn.init.constant_(self.conv3_1.bias, 0)\n",
    "\n",
    "        self.deconv2 = nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1)\n",
    "        self.relu_deconv2 = nn.ReLU(0.3)\n",
    "        nn.init.kaiming_normal_(self.deconv2.weight, a=1.8, mode='fan_out', nonlinearity='relu')\n",
    "        nn.init.constant_(self.deconv2.bias, 0)\n",
    "\n",
    "        self.deconv1 = nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1)\n",
    "        self.relu_deconv1 = nn.ReLU(0.3)\n",
    "        nn.init.kaiming_normal_(self.deconv1.weight, a=1.8, mode='fan_out', nonlinearity='relu')\n",
    "        nn.init.constant_(self.deconv1.bias, 0)\n",
    "\n",
    "        self.deconv0 = nn.ConvTranspose2d(32, 3, kernel_size=4, stride=2, padding=1)\n",
    "        nn.init.kaiming_normal_(self.deconv0.weight, a=1.8, mode='fan_out', nonlinearity='relu')\n",
    "        nn.init.constant_(self.deconv0.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.defc7(x)\n",
    "        x = self.relu_defc7(x)\n",
    "        x = self.defc6(x)\n",
    "        x = self.relu_defc6(x)\n",
    "        x = self.defc5(x)\n",
    "        x = self.relu_defc5(x)\n",
    "        x = x.reshape(x.shape[0],256,4,4)\n",
    "        x = self.deconv5(x)\n",
    "        x = self.relu_deconv5(x)\n",
    "        x = self.conv5_1(x)\n",
    "        x = self.relu_conv5_1(x)\n",
    "        x = self.deconv4(x)\n",
    "        x = self.relu_deconv4(x)\n",
    "        x = self.conv4_1(x)\n",
    "        x = self.relu_conv4_1(x)\n",
    "        x = self.deconv3(x)\n",
    "        x = self.relu_deconv3(x)\n",
    "        x = self.conv3_1(x)\n",
    "        x = self.relu_conv3_1(x)\n",
    "        x = self.deconv2(x)\n",
    "        x = self.relu_deconv2(x)\n",
    "        x = self.deconv1(x)\n",
    "        x = self.relu_deconv1(x)\n",
    "        x = self.deconv0(x)\n",
    "        _, _, height, width = x.size()\n",
    "        start_h = (height - 227) // 2\n",
    "        start_w = (width - 227) // 2\n",
    "        cropped_x = torch.narrow(x, 2, start_h, 227)\n",
    "        cropped_x = torch.narrow(cropped_x, 3, start_w, 227)\n",
    "        \n",
    "        return cropped_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, batch_size):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.Dconv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=7, stride=4)\n",
    "        nn.init.kaiming_normal_(self.Dconv1.weight, a=0.1, mode='fan_in', nonlinearity='relu')\n",
    "        nn.init.constant_(self.Dconv1.bias, 0)\n",
    "        self.Drelu1 = nn.ReLU()\n",
    "\n",
    "        self.Dconv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, stride=1)\n",
    "        nn.init.kaiming_normal_(self.Dconv2.weight, a=1, mode='fan_in', nonlinearity='relu')\n",
    "        nn.init.constant_(self.Dconv2.bias, 0)\n",
    "        self.Drelu2 = nn.ReLU()\n",
    "\n",
    "        self.Dconv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=2)\n",
    "        nn.init.kaiming_normal_(self.Dconv3.weight, a=1, mode='fan_in', nonlinearity='relu')\n",
    "        nn.init.constant_(self.Dconv1.bias, 0)\n",
    "        self.Drelu3 = nn.ReLU()\n",
    "\n",
    "        self.Dconv4 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1)\n",
    "        nn.init.kaiming_normal_(self.Dconv4.weight, a=1, mode='fan_in', nonlinearity='relu')\n",
    "        nn.init.constant_(self.Dconv4.bias, 0)\n",
    "        self.Drelu4 = nn.ReLU()\n",
    "\n",
    "        self.Dconv5 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=2)\n",
    "        nn.init.kaiming_normal_(self.Dconv5.weight, a=1, mode='fan_in', nonlinearity='relu')\n",
    "        nn.init.constant_(self.Dconv5.bias, 0)\n",
    "        self.relu5 = nn.ReLU()\n",
    "\n",
    "        self.Dpool5 = nn.AvgPool2d(kernel_size=11, stride=11)\n",
    "        self.drop5 = nn.Dropout(p=0.5)\n",
    "\n",
    "        self.Dfc6 = nn.Linear(256, 256)\n",
    "        nn.init.kaiming_normal_(self.Dfc6.weight, a=0.1, mode='fan_in', nonlinearity='relu')\n",
    "        nn.init.constant_(self.Dfc6.bias, 0)\n",
    "        self.Drelu6 = nn.ReLU()\n",
    "        self.drop6 = nn.Dropout(p=0.5)\n",
    "\n",
    "        self.Dfc7 = nn.Linear(256, 2)\n",
    "        nn.init.kaiming_normal_(self.Dfc7.weight, a=0.1, mode='fan_in', nonlinearity='relu')\n",
    "        nn.init.constant_(self.Dfc7.bias, 0)\n",
    "    def forward(self, x):\n",
    "        x = self.Dconv1(x)\n",
    "        x = self.Drelu1(x)\n",
    "        x = self.Dconv2(x)\n",
    "        x = self.Drelu2(x)\n",
    "        x = self.Dconv3(x)\n",
    "        x = self.Drelu3(x)\n",
    "        x = self.Dconv4(x)\n",
    "        x = self.Drelu4(x)\n",
    "        x = self.Dconv5(x)\n",
    "        x = self.relu5(x)\n",
    "        x = self.Dpool5(x)\n",
    "        x = x.reshape(x.shape[0],256)\n",
    "        x = self.drop5(x)\n",
    "        x = self.Dfc6(x)\n",
    "        x = self.Drelu6(x)\n",
    "        x = self.drop6(x)\n",
    "        x = self.Dfc7(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Comparator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Comparator, self).__init__()\n",
    "        alexnet = models.alexnet(pretrained=True)\n",
    "\n",
    "        self.alexnet_without_fc = torch.nn.Sequential(*(list(alexnet.features.children())))\n",
    "\n",
    "    def forward(self, x):   \n",
    "        with torch.no_grad():\n",
    "            self.alexnet_without_fc.eval()\n",
    "            features = self.alexnet_without_fc(x)\n",
    "            return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmri_data_table = [\n",
    "    {'subject': 'sub-01',\n",
    "     'data_file': '../data/fmri/sub-01_perceptionNaturalImageTraining_original_VC.h5',\n",
    "     'roi_selector': 'ROI_VC = 1'},\n",
    "    {'subject': 'sub-02',\n",
    "     'data_file': '../data/fmri/sub-02_perceptionNaturalImageTraining_original_VC.h5',\n",
    "     'roi_selector': 'ROI_VC = 1'},\n",
    "    {'subject': 'sub-03',\n",
    "     'data_file': '../data/fmri/sub-03_perceptionNaturalImageTraining_original_VC.h5',\n",
    "     'roi_selector': 'ROI_VC = 1'}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmri_data = np.load('./subj3_fmri.npy')\n",
    "image_features = np.load('./image_features.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = '../data/images/training'\n",
    "image_file_pattern = '*.JPEG'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmri_data_bd = bdpy.BData(fmri_data_table[2]['data_file'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_list = glob.glob(os.path.join(image_dir, image_file_pattern))  # List of image files (full path)\n",
    "images_table = {os.path.splitext(os.path.basename(f))[0]: f\n",
    "                    for f in images_list}\n",
    "label_table = {os.path.splitext(os.path.basename(f))[0]: i + 1\n",
    "                   for i, f in enumerate(images_list)}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmri_labels = fmri_data_bd.get('Label')[:, 1].flatten()\n",
    "fmri_labels = ['n%08d_%d' % (int(('%f' % a).split('.')[0]),\n",
    "                                 int(('%f' % a).split('.')[1]))\n",
    "                   for a in fmri_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, fmri_data, image_features, frmi_labels, images_table):\n",
    "        self.fmri_data = fmri_data\n",
    "        self.image_features = image_features\n",
    "        \n",
    "        self.ridge_regressor = Ridge(alpha=1.0) \n",
    "        self.ridge_regressor.fit(fmri_data, image_features)\n",
    "\n",
    "        self.pred_features = self.ridge_regressor.predict(fmri_data)\n",
    "    def __len__(self):\n",
    "        return len(fmri_data)\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = images_table[fmri_labels[idx]]\n",
    "        preprocess = transforms.Compose([\n",
    "        transforms.Resize(248),\n",
    "        transforms.CenterCrop(227),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "        image = Image.open(image_path)\n",
    "        image = np.asarray(image)\n",
    "        if image.ndim == 2:\n",
    "            img_rgb = np.zeros((image.shape[0], image.shape[1], 3), dtype=image.dtype)\n",
    "            img_rgb[:, :, 0] = image\n",
    "            img_rgb[:, :, 1] = image\n",
    "            img_rgb[:, :, 2] = image\n",
    "            image = img_rgb\n",
    "        input_tensor = preprocess(Image.fromarray(image))\n",
    "        return torch.Tensor(fmri_data[idx]), torch.Tensor(image_features[idx]), torch.Tensor(self.pred_features[idx]),input_tensor\n",
    "traindata = CustomDataset(fmri_data, image_features, fmri_labels, images_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = torch.load('./generator_no_image.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from scipy.stats import pearsonr\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "def calculate_metrics(original_image, predicted_image):\n",
    "    original_image = np.transpose(original_image, (1,2,0))\n",
    "    predicted_image = np.transpose(predicted_image, (1,2,0))\n",
    "    corr_coefficient, _ = pearsonr(original_image.flatten(), predicted_image.flatten())\n",
    "    min_values = np.min(predicted_image, axis=(0, 1))\n",
    "    max_values = np.max(predicted_image, axis=(0, 1))\n",
    "    predicted_image = (predicted_image - min_values)/(max_values-min_values)\n",
    "    ssim_value = ssim(original_image, predicted_image,data_range=1.0, channel_axis=2)\n",
    "    # ssim_value=0\n",
    "    return corr_coefficient, ssim_value\n",
    "\n",
    "def process_batch(batch_original_images, batch_predicted_images):\n",
    "    batch_corr_coefficients = []\n",
    "    batch_ssim_values = []\n",
    "    \n",
    "    for original_image, predicted_image in zip(batch_original_images, batch_predicted_images):\n",
    "        corr_coefficient, ssim_value = calculate_metrics(original_image, predicted_image)\n",
    "        batch_corr_coefficients.append(corr_coefficient)\n",
    "        batch_ssim_values.append(ssim_value)\n",
    "\n",
    "    \n",
    "    return batch_corr_coefficients, batch_ssim_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "generator.eval()\n",
    "batch_size = 1200\n",
    "data_loader = DataLoader(traindata, batch_size=batch_size)\n",
    "print(len(data_loader))\n",
    "pearsons = []\n",
    "ssims = []\n",
    "with torch.no_grad():\n",
    "    for i, (fmri_data_batch,_,pred_features,original_image) in enumerate(data_loader):\n",
    "        # generated_image = generator(torch.concat((fmri_data_batch, pred_features), dim=1))\n",
    "        generated_image = generator(fmri_data_batch)\n",
    "\n",
    "        pearson, ssim_batch = process_batch(original_image.numpy(), generated_image.numpy())\n",
    "        pearsons.append(np.mean(pearson))\n",
    "        ssims.append(np.mean(ssim_batch))\n",
    "print(\"Pearson Correlation: \", np.mean(pearsons))\n",
    "print(\"SSIM: \", np.mean(ssims))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.eval()\n",
    "num_images = 50\n",
    "fig, axes = plt.subplots(num_images, 2, figsize=(10, 2*num_images))\n",
    "batch_size = 1200\n",
    "data_loader = DataLoader(traindata, batch_size=batch_size)\n",
    "with torch.no_grad():\n",
    "    for i, (fmri_data_batch,_,pred_features,original_image) in enumerate(data_loader):\n",
    "        generated_image = generator(torch.concat((fmri_data_batch, pred_features), dim=1))\n",
    "        for j in range(100,150):\n",
    "            i = j - 100\n",
    "            axes[i, 0].imshow(np.transpose(original_image[i], (1, 2, 0)))\n",
    "            axes[i, 0].set_title('Original Image')\n",
    "            axes[i, 0].axis('off')\n",
    "            generated_image_curr = np.transpose(generated_image[i], (1, 2, 0)).numpy()\n",
    "            # generated_image = generated_image - generated_image.min()\n",
    "            # generated_image = generated_image * (1/generated_image.max())\n",
    "            # mean = np.mean(generated_image[i], axis=(0, 1))\n",
    "            # std = np.std(generated_image[i], axis=(0, 1))\n",
    "            min_values = np.min(generated_image_curr, axis=(0, 1))\n",
    "            max_values = np.max(generated_image_curr, axis=(0, 1))\n",
    "            # print(mean, std)\n",
    "            axes[i, 1].imshow((generated_image_curr - min_values)/(max_values-min_values))\n",
    "            axes[i, 1].set_title('Generated Image')\n",
    "            axes[i, 1].axis('off')\n",
    "        break\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generated_image[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(traindata[0][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmri_data_table_test = [\n",
    "    {'subject': 'sub-01',\n",
    "     'data_file': '../data/fmri/sub-01_perceptionNaturalImageTraining_original_VC.h5',\n",
    "     'roi_selector': 'ROI_VC = 1'},\n",
    "    {'subject': 'sub-02',\n",
    "     'data_file': '../data/fmri/sub-02_perceptionNaturalImageTraining_original_VC.h5',\n",
    "     'roi_selector': 'ROI_VC = 1'},\n",
    "    {'subject': 'sub-03',\n",
    "     'data_file': '../data/fmri/sub-03_perceptionNaturalImageTest_original_VC.h5',\n",
    "     'roi_selector': 'ROI_VC = 1'}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmri_data_bd = bdpy.BData(fmri_data_table_test[2]['data_file'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir_test = '../data/images/test'\n",
    "image_file_pattern = '*.JPEG'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_list_test = glob.glob(os.path.join(image_dir_test, image_file_pattern))  # List of image files (full path)\n",
    "images_table_test = {os.path.splitext(os.path.basename(f))[0]: f\n",
    "                    for f in images_list_test}\n",
    "label_table_test = {os.path.splitext(os.path.basename(f))[0]: i + 1\n",
    "                   for i, f in enumerate(images_list_test)}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmri_labels_test = fmri_data_bd.get('Label')[:, 1].flatten()\n",
    "fmri_labels_test = ['n%08d_%d' % (int(('%f' % a).split('.')[0]),\n",
    "                                 int(('%f' % a).split('.')[1]))\n",
    "                   for a in fmri_labels_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmri_data_test = np.load('subj3_fmri_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDatasetTest(Dataset):\n",
    "    def __init__(self, fmri_data,fmri_labels, images_table):\n",
    "        self.fmri_data = fmri_data\n",
    "        self.fmri_labels = fmri_labels\n",
    "        self.images_table = images_table\n",
    "        self.pred_features = traindata.ridge_regressor.predict(fmri_data)\n",
    "    def __len__(self):\n",
    "        return len(self.fmri_data)\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.images_table[self.fmri_labels[idx]]\n",
    "        preprocess = transforms.Compose([\n",
    "        transforms.Resize(248),\n",
    "        transforms.CenterCrop(227),\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "        image = Image.open(image_path)\n",
    "        image = np.asarray(image)\n",
    "        if image.ndim == 2:\n",
    "            img_rgb = np.zeros((image.shape[0], image.shape[1], 3), dtype=image.dtype)\n",
    "            img_rgb[:, :, 0] = image\n",
    "            img_rgb[:, :, 1] = image\n",
    "            img_rgb[:, :, 2] = image\n",
    "            image = img_rgb\n",
    "        input_tensor = preprocess(Image.fromarray(image))\n",
    "        return torch.Tensor(self.fmri_data[idx]),torch.Tensor(self.pred_features[idx]),input_tensor\n",
    "testdata = CustomDatasetTest(fmri_data_test,fmri_labels_test, images_table_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fmri_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.eval()\n",
    "batch_size = 1200\n",
    "data_loader = DataLoader(testdata, batch_size=batch_size)\n",
    "pearsons = []\n",
    "ssims = []\n",
    "with torch.no_grad():\n",
    "    for i, (fmri_data,pred_features,original_image) in enumerate(data_loader):\n",
    "        generated_image = generator(torch.concat((fmri_data,pred_features), dim=1))\n",
    "        pearson, ssim_batch = process_batch(original_image.numpy(), generated_image.numpy())\n",
    "        pearsons.append(np.mean(pearson))\n",
    "        ssims.append(np.mean(ssim_batch))\n",
    "print(\"Pearson Correlation: \", np.mean(pearsons))\n",
    "print(\"SSIM: \", np.mean(ssims))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.eval()\n",
    "num_images = 50\n",
    "fig, axes = plt.subplots(num_images, 2, figsize=(10, 2*num_images))\n",
    "batch_size = 1200\n",
    "data_loader = DataLoader(testdata, batch_size=batch_size)\n",
    "with torch.no_grad():\n",
    "    for i, (fmri_data,pred_features,original_image) in enumerate(data_loader):\n",
    "        generated_image = generator(torch.concat((fmri_data, pred_features), dim=1))\n",
    "        for j in range(100,150):\n",
    "            i = j - 100\n",
    "            axes[i, 0].imshow(np.transpose(original_image[i], (1, 2, 0)))\n",
    "            axes[i, 0].set_title('Original Image')\n",
    "            axes[i, 0].axis('off')\n",
    "            generated_image_curr = np.transpose(generated_image[i], (1, 2, 0)).numpy()\n",
    "            # generated_image = generated_image - generated_image.min()\n",
    "            # generated_image = generated_image * (1/generated_image.max())\n",
    "            # mean = np.mean(generated_image[i], axis=(0, 1))\n",
    "            # std = np.std(generated_image[i], axis=(0, 1))\n",
    "            min_values = np.min(generated_image_curr, axis=(0, 1))\n",
    "            max_values = np.max(generated_image_curr, axis=(0, 1))\n",
    "            \n",
    "            # print(mean, std)\n",
    "            pct = 0.04\n",
    "            generated_image_curr[:,:,0] = np.clip(generated_image_curr[:,:,0], np.percentile(generated_image_curr[:,:,0], pct/2.),\n",
    "                  np.percentile(generated_image_curr[:,:,0], 100-pct/2.))\n",
    "            generated_image_curr[:,:,1] = np.clip(generated_image_curr[:,:,1], np.percentile(generated_image_curr[:,:,1], pct/2.),\n",
    "                  np.percentile(generated_image_curr[:,:,1], 100-pct/2.))\n",
    "            generated_image_curr[:,:,2] = np.clip(generated_image_curr[:,:,2], np.percentile(generated_image_curr[:,:,2], pct/2.),\n",
    "                  np.percentile(generated_image_curr[:,:,2], 100-pct/2.))\n",
    "            generated_image_curr = (generated_image_curr - min_values)/(max_values-min_values)\n",
    "            axes[i, 1].imshow((generated_image_curr))\n",
    "            axes[i, 1].set_title('Generated Image')\n",
    "            axes[i, 1].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_table = [\n",
    "    # {\n",
    "    #     'subject': 'sub-01',\n",
    "\n",
    "    #     # The file of the test fmri data\n",
    "    #     'data_fmri_test': './data/fmri/sub-01_perceptionNaturalImageTest_original_VC.h5',\n",
    "\n",
    "    #     # The file of the training fmri data\n",
    "    #     'data_fmri_training': './data/fmri/sub-01_perceptionNaturalImageTraining_original_VC.h5',\n",
    "\n",
    "    #     # Path to caffemodel trained generator (caffemodel and prototxt files).\n",
    "    #     # 'generator_caffemodel' can be either path to a caffemodel file or\n",
    "    #     # a directory that contains snapshots of trained models.\n",
    "    #     'generator_caffemodel': './net_pretrained/sub-01/generator.caffemodel',\n",
    "    #     'generator_prototxt': './net_pretrained/sub-01/generator_test.prototxt',\n",
    "    #     # 'generator_caffemodel': './net_trained/sub-01/snapshots',\n",
    "    #     # 'generator_prototxt': './net_trained/sub-01/net/generator_test.prototxt',\n",
    "\n",
    "    #     # Data select expression specifying columns (ROIs) used as the test and training data\n",
    "    #     'roi_selector': 'ROI_VC = 1',\n",
    "\n",
    "    #     # Output directory\n",
    "    #     'output_dir': os.path.join(output_dir_base, 'sub-01')\n",
    "    # },\n",
    "    # {\n",
    "    #     'subject': 'sub-02',\n",
    "    #     'data_fmri_test': './data/fmri/sub-02_perceptionNaturalImageTest_original_VC.h5',\n",
    "    #     'data_fmri_training': './data/fmri/sub-02_perceptionNaturalImageTraining_original_VC.h5',\n",
    "    #     'generator_caffemodel': './net_pretrained/sub-02/generator.caffemodel',\n",
    "    #     'generator_prototxt': './net_pretrained/sub-02/generator_test.prototxt',\n",
    "    #     # 'generator_caffemodel': './net_trained/sub-02/snapshots',\n",
    "    #     # 'generator_prototxt': './net_trained/sub-02/net/generator_test.prototxt',\n",
    "    #     'roi_selector': 'ROI_VC = 1',\n",
    "    #     'output_dir': os.path.join(output_dir_base, 'sub-02')\n",
    "    # },\n",
    "    {\n",
    "        'subject': 'sub-03',\n",
    "        'data_fmri_test': '../data/fmri/sub-03_perceptionNaturalImageTest_original_VC.h5',\n",
    "        'data_fmri_training': '../data/fmri/sub-03_perceptionNaturalImageTraining_original_VC.h5',\n",
    "        'generator_caffemodel': './trainedmodel_sub-03/generator.caffemodel',\n",
    "        'generator_prototxt': './trainedmodel_sub-03/generator_test.prototxt',\n",
    "        # 'generator_caffemodel': './net_trained/sub-03/snapshots',\n",
    "        # 'generator_prototxt': './net_trained/sub-03/net/generator_test.prototxt',\n",
    "        'roi_selector': 'ROI_VC=1',\n",
    "        # 'output_dir': os.path.join(output_dir_base, 'sub-03')\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = data_table[0]\n",
    "fmri_data_test_file = dat['data_fmri_test']\n",
    "fmri_data_selector = dat['roi_selector']\n",
    "fmri_data_training_file = dat['data_fmri_training']\n",
    "fmri_data = bdpy.BData(fmri_data_test_file)\n",
    "brain_data = fmri_data.select(fmri_data_selector)\n",
    "brain_labels = fmri_data.select('Label = 1')[:, 1]\n",
    "\n",
    "\n",
    "fmri_data_train = bdpy.BData(fmri_data_training_file)\n",
    "brain_data_train = fmri_data_train.select(fmri_data_selector)\n",
    "\n",
    "\n",
    "brain_data_mean = np.mean(brain_data_train, axis=0)\n",
    "brain_data_norm = np.std(brain_data_train, axis=0)\n",
    "\n",
    "del(fmri_data_train)\n",
    "del(brain_data_train)\n",
    "\n",
    "# Average fMRI data across trials\n",
    "brain_data_ave = []\n",
    "image_list = []\n",
    "\n",
    "labels_set = np.unique(brain_labels)\n",
    "for lb in labels_set:\n",
    "    sample_index = brain_labels == lb\n",
    "    brain_data_sample = brain_data[sample_index, :]\n",
    "    brain_data_ave.append(np.mean(brain_data_sample, axis=0))\n",
    "\n",
    "    # Convert stimulus ID to stimulus file name\n",
    "    image_name = 'n%08d_%d' % (int(('%f' % lb).split('.')[0]),\n",
    "                                int(('%f' % lb).split('.')[1]))\n",
    "    image_list.append(image_name)\n",
    "\n",
    "brain_data_ave = np.vstack(brain_data_ave)\n",
    "\n",
    "# Normalize fMRI data\n",
    "brain_data_ave = (brain_data_ave - brain_data_mean) / brain_data_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 4.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, img in enumerate(image_list):\n",
    "    input_data = brain_data_ave[i, :] * scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics2(original_image, predicted_image):\n",
    "    original_image = np.transpose(original_image.numpy(), (1,2,0))\n",
    "    corr_coefficient, _ = pearsonr(original_image.flatten(), predicted_image.flatten())\n",
    "    ssim_value = ssim(original_image, predicted_image,data_range=1.0, channel_axis=2)\n",
    "    # ssim_value=0\n",
    "    return corr_coefficient, ssim_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.eval()\n",
    "num_images = 50\n",
    "pearsons = []\n",
    "ssims = []\n",
    "fig, axes = plt.subplots(num_images, 2, figsize=(10, 2*num_images))\n",
    "with torch.no_grad():\n",
    "    for i, image_path in enumerate(image_list):\n",
    "        name = image_path\n",
    "        image_path = '../data/images/test/' + image_path + '.JPEG'\n",
    "        preprocess = transforms.Compose([\n",
    "        transforms.Resize(248),\n",
    "        transforms.CenterCrop(227),\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "        image = Image.open(image_path)\n",
    "        image = np.asarray(image)\n",
    "        if image.ndim == 2:\n",
    "            img_rgb = np.zeros((image.shape[0], image.shape[1], 3), dtype=image.dtype)\n",
    "            img_rgb[:, :, 0] = image\n",
    "            img_rgb[:, :, 1] = image\n",
    "            img_rgb[:, :, 2] = image\n",
    "            image = img_rgb\n",
    "        original_image = preprocess(Image.fromarray(image))\n",
    "        input_data = brain_data_ave[i, :] * scale\n",
    "        pred_features = traindata.ridge_regressor.predict(input_data.reshape(1,-1))\n",
    "        generated_image = generator(torch.concat((torch.tensor(input_data, dtype=torch.float).unsqueeze(0), torch.tensor(pred_features, dtype=torch.float)), dim = 1))\n",
    "        axes[i, 0].imshow(np.transpose(original_image, (1, 2, 0)))\n",
    "        axes[i, 0].set_title('Original Image')\n",
    "        axes[i, 0].axis('off')\n",
    "        generated_image_curr = np.transpose(generated_image[0], (1, 2, 0)).numpy()\n",
    "        # generated_image = generated_image - generated_image.min()\n",
    "        # generated_image = generated_image * (1/generated_image.max())\n",
    "        # mean = np.mean(generated_image[i], axis=(0, 1))\n",
    "        # std = np.std(generated_image[i], axis=(0, 1))\n",
    "        min_values = np.min(generated_image_curr, axis=(0, 1))\n",
    "        max_values = np.max(generated_image_curr, axis=(0, 1))\n",
    "        \n",
    "        # print(mean, std)\n",
    "        pct = 0.04\n",
    "        generated_image_curr[:,:,0] = np.clip(generated_image_curr[:,:,0], np.percentile(generated_image_curr[:,:,0], pct/2.),\n",
    "                np.percentile(generated_image_curr[:,:,0], 100-pct/2.))\n",
    "        generated_image_curr[:,:,1] = np.clip(generated_image_curr[:,:,1], np.percentile(generated_image_curr[:,:,1], pct/2.),\n",
    "                np.percentile(generated_image_curr[:,:,1], 100-pct/2.))\n",
    "        generated_image_curr[:,:,2] = np.clip(generated_image_curr[:,:,2], np.percentile(generated_image_curr[:,:,2], pct/2.),\n",
    "                np.percentile(generated_image_curr[:,:,2], 100-pct/2.))\n",
    "        generated_image_curr = (generated_image_curr - min_values)/(max_values-min_values)\n",
    "        axes[i, 1].imshow((generated_image_curr))\n",
    "        axes[i, 1].set_title('Generated Image')\n",
    "        axes[i, 1].axis('off')\n",
    "        curr_pearson, curr_ssim = calculate_metrics2(original_image, generated_image_curr)\n",
    "        img = Image.fromarray((generated_image_curr *255).astype(np.uint8), \"RGB\")\n",
    "        # img.save(f'./results/baseline/{name}.jpeg')\n",
    "        pearsons.append(curr_pearson)\n",
    "        ssims.append(curr_ssim)\n",
    "print(\"Pearson Correlation: \", np.mean(pearsons))\n",
    "print(\"SSIM: \", np.mean(ssims))\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pearsons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics3(original_image, predicted_image):\n",
    "    corr_coefficient, _ = pearsonr(original_image.flatten(), predicted_image.flatten())\n",
    "    ssim_value = ssim(original_image, predicted_image,data_range=1.0, channel_axis=2)\n",
    "    # ssim_value=0\n",
    "    return corr_coefficient, ssim_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "def load_images_from_folder(folder_path):\n",
    "    pearsons = []\n",
    "    ssims =  []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        \n",
    "        image_name = filename.split(\".\")[0][7:]\n",
    "        img = Image.open(\"./results/\" + filename)\n",
    "        predicted = np.asarray(img)\n",
    "        img = Image.open(\"../data/images/test/\" + image_name + \".JPEG\").resize((227,227))\n",
    "        original = np.asarray(img)\n",
    "        pearson_curr, ssim_curr = calculate_metrics3(original, predicted)\n",
    "        pearsons.append(pearson_curr)\n",
    "        ssims.append(ssim_curr)\n",
    "        plt.imshow(original)\n",
    "        plt.imshow(predicted)\n",
    "    print(\"Pearson Correlation: \", np.mean(pearsons))\n",
    "    print(\"SSIM: \", np.mean(ssims))\n",
    "folder_path = \"./results/\"\n",
    "images = load_images_from_folder(folder_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
